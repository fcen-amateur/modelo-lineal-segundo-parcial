---
title: "Segundo Parcial - Ejercicio Practico"
author: "Gonzalo Barrera Borla"
date: "July 3, 2019"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # Manipulacion de data frames y graficas amenas
library(broom) # Simplifica el agregado de predichos, residuos y demases a un data frame
library(MASS) # Box-Cox
```

Comezamos cargando los datos y estimando el modelo completo:

```{r a_carga__modelo_completo}
datos1 <- read.table("datos1.txt")
n <- dim(datos1)[1]

df <- as_tibble(datos1, rownames = "obs")

# a)
modelo_completo <- y ~ x1 + x2 + x3 + x4
llamada_lm <- lm(modelo_completo, data = df)
summary(llamada_lm)
df <- broom::augment(llamada_lm)
```

```{r b_residuos_std_versus_predichos}
# b grafico residuos estandarizados (estudentizados internos) versus predichos
(grafico_b <- ggplot(df, aes(x=.fitted, y=.std.resid)) +
  geom_point())
```

Esperaria una nube de puntos y encuentro una sonrisa! Evidentemente, el supuesto de homocedasticidad _ha sido violado_.

```{r c_qq_plot_residuos_std}
qqnorm(df$.std.resid)
```

```{r cbis_qqplot_casero}
(grafico_cbis <- df %>%
  arrange(.std.resid) %>%
  mutate(
    cuantil_teorico = qnorm((seq(n)-0.5)/n)) %>%
  ggplot(aes(cuantil_teorico, .std.resid)) +
  geom_point() +
  geom_abline(slope=1, intercept=0, color='gray'))
```


Se observan colas pesadas, sobre todo a izquierda, lo cual nos hace sospechar de la normalidad de los residuos. Un test conocido es el de Shapiro-Wilk, asi que lo aplicamos. Tomamos $\alpha = 0.05$ por costumbre:

```{r normalidad_residuos}
alfa <- 0.05
llamada_shapiro <- shapiro.test(df$.std.resid)
```

El test nos da un p-valor de `r llamada_shapiro$p.value`, con lo cual `r ifelse(llamada_shapiro$p.value < alfa, "SÍ", "NO")` tenemos suficiente evidencia como para rechazar la hipótesis nula de que los datos tienen una distribución subyacente normal.

Cuando tenemos evidencia tan fuerte de la no-normalidad de los errores (y por ende de la variable respuesta), un potencial "arreglo" consiste en postular una familia de transformaciones (ya sea para la respuesta y/o las covariables), y encontrar dentro de ellas la que "mejor normaliza" los datos. En particular, proponemos la familia de transformaciones Box-Cox, para la respuesta 
$$
y^{(\lambda)}=g(y,\lambda)=\begin{cases}
       \ln(y) &\quad \text{si } \lambda=0 \\
       (y^{\lambda}-1) /{\lambda} &\quad\text{si } \lambda \neq 0\\
       \end{cases}
$$

```{r d_boxcox}
llamada_boxcox <- boxcox(modelo_completo, data = df)
lambda <- llamada_boxcox$x[which.max(llamada_boxcox$y)]
```

El gráfico que devuelve la llamada a `MASS::boxcox`, nos provee un intervalo de confianza de nivel 0.05 para $\lambda$, de manera que ser válida la transformación, el "verdadero $\lambda$" debería estar dentro de este intervalo en 95 de cada 100 ajustes. Se ve que la log-verosimilitud tiene un pico muy marcado alrededor de `r lambda`, con lo cual _no_ da igual qué $\lambda$ se use en la transformacion, y este sea tal vez un buen candidato. Transformemos $y$ y veamos cómo ajusta el nuevo modelo y el correspondiente grafico de los residuos:

```{r d_modelo_transformado}
g_boxcox <- function(y, lambda) {
  if (lambda == 0) {
    return(log(y))
  } else {
    return((y^lambda - 1) / lambda)
  }
}

df_lambda <- as_tibble(datos1, rownames = "obs") %>%
  mutate(y_lambda = g_boxcox(y, lambda))

modelo_transformado <- y_lambda ~ x1 + x2 + x3 + x4
llamada_lm_transformado <- lm(modelo_transformado, df_lambda)
df_lambda <- augment(llamada_lm_transformado)
(grafico_d <- ggplot(df_lambda, aes(x=.fitted, y=.std.resid)) +
  geom_point())

(grafico_dbis <- df_lambda %>%
  arrange(.std.resid) %>%
  mutate(
    cuantil_teorico = qnorm((seq(n)-0.5)/n)) %>%
  ggplot(aes(cuantil_teorico, .std.resid)) +
  geom_point() +
  geom_abline(slope=1, intercept=0, color='gray'))

llamada_shapiro_lambda <- shapiro.test(df_lambda$.std.resid)
```

Ya no es tan evidente que exista estructura en los residuos estandarizados, y el p-valor del test de Shapiro (`r llamada_shapiro$p.value`), nos dice que al mismo nivel de antes, `r ifelse(llamada_shapiro_lambda$p.value < alfa, "SÍ", "NO")` tenemos suficiente evidencia para rechazar la hipotesis nula de normalidad. En consecuencia, proseguiremos el analisis con el modelo transformado.

La medida $C_p$ de Mallow para el modelo $i$ se define como 
$$
C_{p,i}\frac{RSS_{p,i}}{s^2}+2p-n
$$

donde $RSS_{p,i}$ es la RSS para el modelo i que contiene p variables, y $s^2$ es el estimador tradicional de $\sigma^2$, calculado usando el modelo con todas las variables disponibles. 
Alternativamente, $C_{p,i}$ se puede expresar en funcion de los $R^2$ ajustados (cf. Seber, 12.3.2), y eso usaremos:
```{r e_cp_mallows}
covariables <- c("x1","x2","x3","x4")

generar_formulas <- function(p, y_nombre, X_nombre) {
  lados_derechos <- unlist(
      combn(x = X_nombre, m = p,
            simplify = F, FUN = paste, collapse = " + "))
  
  map(str_c(y_nombre, "~", lados_derechos, sep = " "), as.formula)
}

modelos <- map(seq_len(4), 
                generar_formulas,
                y_nombre = "y_lambda",
                X_nombre = covariables) %>%
  imap(~ tibble("p" = .y, "modelo" = .x)) %>%
  bind_rows %>%
  mutate(nombre = map(modelo, deparse) %>% map_chr(str_c, collapse = ""))

modelos <- modelos %>%
  mutate(
    ajuste = map(modelo, lm, data=df_lambda),
    resumen = map(ajuste, glance),
    adj_r_sq = map_dbl(resumen, "adj.r.squared"))

adj_r_sq_full <- modelos %>%
  filter(nombre == "y_lambda ~ x1 + x2 + x3 + x4") %>%
  pluck("adj_r_sq")

calcular_cp <- function(adj_r_sq, p, adj_r_sq_full, n) {
  return((1-adj_r_sq)*(n-p)/(1-adj_r_sq_full)-n+2*p)
}

modelos <- modelos %>%
  mutate(
    cp = map2_dbl(adj_r_sq, p, calcular_cp, adj_r_sq_full=adj_r_sq_full, n=n)
  )

mejor_modelo <- top_n(modelos, 1, -cp)
```

Este procedimiento, aplicado a todos los modelos, nos recomienda elegir `r mejor_modelo$nombre`, que tiene un $C_p$ de `r mejor_modelo$cp`, ligeramente mejor que el modelo completo.

Si siguieramos un criterio de _selección hacia adelante_, deberíamos ir agregando variables de a una al modelo "base" únicamente con ordenada, siempre y cuando $R^2_{adj,p+1} \geq R^2_{adj,p}$. Reutilizando la tabla del punto anterior:

```{r f_forward}
modelos %>%
  dplyr::select(nombre, adj_r_sq, cp)
```

Se ve que la primera covariable a agregar debería ser $x_1$, seguida por $x_3$.

```{r g_corr_matrix}
(matriz_corr <- cor(df_lambda[,covariables]))
```

Es notorio que las variables $x_1,x_2$ estan fuertemente correlacionadas, y por ende tal vez no sea del todo sabio 